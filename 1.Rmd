---
title: "1"
author: "Siming Yan"
date:   "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: spacelab
    keep_md: true
    toc: yes
    toc_float: true
    highlight: haddock
    
---

<html>
<style> 
div.bgm { background-color:#e6fff0; border-radius: 7px; padding: 10px;} 
.ans {
  color: purple;
  font-weight: bold;
}
#main { 
    color: #2cb061; 
}
</style>

<div class = "bgm">

<ul id="main">

```{r setup, include=FALSE}
options(max.print="75")
knitr::opts_chunk$set(
	fig.width = 7,
	message = FALSE,
	warning = FALSE,
	cache = TRUE,
	comment = NA,
	dpi = 300,
	prompt = FALSE,
	tidy = TRUE
)
# opts_knit$set(width=75)
# knitr::opts_chunk$set()
options(digits=5)
options(scipen=5)
knitr::opts_chunk$set(warning = F, message=F)
```

```{r, include=FALSE}
kbt <- function(...){
  knitr::kable(..., format.args = list(big.mark = ',', scientific = F)) %>%
    kableExtra::kable_styling(c("striped", "hover", "condensed"),
                              full_width = F,
                              position = "center",
                              row_label_position = "c") %>%
    row_spec(0, bold = T, color = "white", background = "#004d1f")
}

gpt <- function(...){
  ggplot2::ggplot(...) + 
    theme_minimal() +
    theme(plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
          plot.subtitle = element_text(face = "italic", size = 12, hjust = 0.5), 
          axis.text.x = element_text(angle = 0, hjust = 0.5))
}

stg <- function(...){
  stargazer::stargazer(..., 
                       type = "text",
                       # style = "qje",
                       # title = "daily_kwh ~ post:encouraged|customer_id + no.bill|0|customer_id",
                       keep.stat = c("n", "ser"),
                       digits = 5)
}
#




multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}


#--------------------------------------------

library(parallel)
library(doMC)
numCores <- detectCores()
registerDoMC(8)
```

## Data preview

```{r cars, include =F}
data <- read.csv("./test_1.csv")
head(data)

names(data)[1] = "y"

```

```{r, echo=F}
library(kableExtra)
library(tidyverse)
library(mice)
md.pattern(data)
```
  

```{r, echo =F}
library(summarytools)
dfSummary(data) %>% view
descr(data) %>% data.frame()

plotfunc = function(...){
  gpt(data) + aes(x = ..., y = y) +
    geom_point() + geom_line()
}

multiplot(plotfunc(X1), plotfunc(X2), plotfunc(X3),
          plotfunc(X4), plotfunc(X5), plotfunc(X6),cols = 2)
```

## models:

```{r, include =FALSE}
library(knitr)
library(readr)
library(stringr)
library(data.table)
library(naniar)
library(scales)

library(trelliscopejs)
library(caret)
library(ranger)
library(neuralnet)
library(plm)
library(gamlr)
library(kernlab)
library(glmnet)
library(e1071)
library(dplyr)
library(gbm)
library(causalTree)
library(grf)
```

## sample splitting

```{r}
samplesize = 0.75 * nrow(data)
set.seed(10)
index = sample(nrow(data),size = samplesize)

# creating training and test set
train = data[index,]
test = data[-index,]
```

### PCA (7.0.12)

```{r PCA}

tuneGrid_pcr = data.frame(.ncomp = c(4:7))

pcr_model <- train(
  y ~ .,
  tuneLength = 1,
  data = train, 
  method = "pcr",
  tuneGrid = tuneGrid_pcr,
  trControl = trainControl(
    method = "cv", 
    number = 10, 
    verboseIter = FALSE
  )
)

# pcr_model
```

```{r, echo = FALSE}

# png("Rplot%03d.png", width = 720)
pcr_model
# dev.off()
#  6      0.053982  0.58495   0.039139
```
 

### random forests

```{r RF, comment = F}
library(doParallel)
cl <- makePSOCKcluster(8)
registerDoParallel(cl)


# From previous step
tuneGrid <- data.frame(
  .mtry = c(1:3)
  # .splitrule = "variance",
  # .min.node.size = 5
)

# Fit random forest: model
rf_model <- train(
  y ~ .-X6,
  tuneLength = 1,
  data = train, 
  method = "rf",
  tuneGrid = tuneGrid,
  trControl = trainControl(
    method = "cv", 
    number = 10, 
    verboseIter = FALSE
  )
)
```

```{r, comment= F, echo = F}
rf_model
# 0.033427
## All subsequent models are then run in parallel
# model <- train(y ~ ., data = training, method = "rf")

## When you are done:
stopCluster(cl)
```

```{r RF_P, echo = F}
pred_y <- predict(rf_model, newdata = test)
final <- as.data.frame(cbind(pred_y,test$y))
names(final) <- c("pred_y","real_y")
prf <- ggplot(final, aes(x=pred_y,y=real_y)) + 
  geom_point(alpha=0.65, col ="grey") + 
  scale_x_continuous(limits=c(0,1)) +
  scale_y_continuous(limits=c(0,1)) +
  geom_abline(intercept=0,slope=1) +
  geom_smooth(se=0, smethod = "lm") +
  coord_fixed(1) +
  ggtitle("RF")
```

```{r, echo =F}
prf
```

### SGB

```{r SGB, comment = F}
# From previous step
tuneGrid <- data.frame(
  .n.trees = 9999,
  .interaction.depth = c(8:16),
  .shrinkage = 0.01,
  .n.minobsinnode = 6
)

# Stochastic boosting
sgb_model <- train(
  y ~ .,
  tuneLength = 1,
  data = train, 
  method = 'gbm',
  tuneGrid = tuneGrid,
  trControl = trainControl(
    method = "cv", 
    number = 10, 
    verboseIter = FALSE
  )
)

# Print model to console
# sgb_model
sgb_model$bestTune

# sgb_sav_3_8_16 = sgb_model
# sgb_sav_99_8_16 = sgb_model

# png("Rplot%03d.png", width = 720)
plot(sgb_model)
# dev.off()
```

```{r, echo=F}
pr_sgb <- predict(sgb_model, newdata = test)
final_sgb <- as.data.frame(cbind(pr_sgb, test$y))
names(final) <- c("pred_y","real_y")
psgb <- ggplot(final, aes(x=pred_y,y=real_y)) + 
  geom_point(alpha=0.65, col ="grey") + 
  scale_x_continuous(limits=c(0,1)) +
  scale_y_continuous(limits=c(0,1)) +
  geom_abline(intercept=0,slope=1) +
  geom_smooth(se=0,method = "lm") +
  coord_fixed(1)+
  ggtitle("Stochastic Gradient Boosting")
psgb
```

```{r, echo = F}
sgb_model

#0.034641	
```

### NN

```{r nn }
library(parallel)
library(doMC)
numCores <- detectCores()
registerDoMC(8)


tuneGrid <- data.frame(layer1 = 12, layer2 = 6, layer3 = 1)
nn_model <- train(
  y ~ .,
  tuneLength = 1,
  data = train, 
  method = 'neuralnet',
  tuneGrid = tuneGrid,
  trControl = trainControl(
    method = "cv", 
    number = 10, 
    verboseIter = FALSE
  )
)
```

```{r nn_out}
# compute(nn_model, data.frame(test))
pred_nn <- predict(nn_model, newdata = test)
final <- cbind(pred_nn, test$y) %>% data.frame()
 

names(final) <- c("pred_y","real_y")

pnn <- ggplot(final, aes(x=pred_y, y=real_y)) + 
  geom_point(alpha=0.65, col ="grey") + 
  scale_x_continuous(limits=c(0,1)) +
  scale_y_continuous(limits=c(0,1)) +
  geom_abline(intercept=0,slope=1) +
  geom_smooth(se=0, smethod = "lm") +
  coord_fixed(1) +
  ggtitle("Neural Network")

pnn

```

```{r}
nn_model
```
## results 

```{r}
results = data.frame(names = "RMSE",
           PCA = "0.053982",
           SGB = "0.034641",
           RF = "0.033427",
           NN = "0.05182")

kbt(results)
```


```{r}
df = read.csv("/Users/fyenne/Downloads/booooks/semester5/tensor/ucp.csv")
```


```{r}
B = 1000
mub = c()
for (b in 1:B){
  samp_b = sample.int(nrow(df), replace = T)
  mub = c(mub, mean(df$Price[samp_b]))
}
```

  
```{r}
library(tidyverse)
df %>% nrow()
sample.int(nrow(df), replace = T)

c(mub, mean(df$Price[2313]))

df$Price[2313]
```

```{r}
df = read.csv("./ch6problem14.csv")
# scan(file.choose(""))
df
acf(df)
pacf(df)
df[,c(2:4)]
chisq.test(df[,c(4)])
```


```{r}
library(fitdistrplus)
plotdist(df, discrete = T)
df %>% ggplot(aes(x = Period, y = N, fill = Day)) + geom_col()


plot(df %>% dplyr::select(c("Period","N")))
fp = fitdist(df$N, "pois")
summary(fp)

acf(df$N)
```

```{r}
fp = fitdist(df$N, "pois")
summary(fp)
gofstat(fp) 
```

```{r}
xta = xtabs(~N+ as.factor(Day), data = df) 
chisq.test(xta)
```

```{r}
# d.ergo <- data.frame(Type = paste0("T", rep(1:4, 9*4)),
#                       Subj = gl(9, 4, 36*4))
# d.ergo
# xtabs(~ Type + Subj, data = d.ergo)
```

